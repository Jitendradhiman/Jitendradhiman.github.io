{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Logistic Regression (MLR)\n",
    "We will implement multiclass logistic regression from scratch using cross entrpoy loss with sigmoid function that models the class probabilities.<br>\n",
    "Although, we describe the formulation of binary class clasification, we code multiclass case. The link between binary class logistic regression and its multiclass counterpart can be drawn by using one-vs-rest strategy (see point 5 below).<br>\n",
    "Settings for binary class logistic regression<br>\n",
    "$\\{x_i,y_i\\}_{i=1}^{N}$; $x_i \\in \\mathbb{R}^{p\\times 1}$ where $y_i$ is $1$ if data point $x_i$ belongs to class $1$, otherwise it is zero.<br>\n",
    "1. Sigmoid function <br> \n",
    "$g(z) = \\frac{1}{1+e^{-z}}$ <br>\n",
    "2. $h_{\\theta}(x_i) = g(\\theta^Tx_i)$ is the predicted probability that the input $x$ is being classified \"positive\"\n",
    "3. Data log-liklihood <br>\n",
    "$\n",
    "J(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\bigg[-y_i \\text{log} h_{\\theta}(x_i) - (1-y_i) \n",
    "\\text{log} (1-h_{\\theta}(x_i))\\bigg]\n",
    "$\n",
    "4. Derivative of log-likelihood with respect to $\\theta_j$<br>\n",
    "$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{N}\\sum_{i=1}^{N}(h_{\\theta}(x_i)-y_i)x_{ij}\n",
    "$\n",
    "5. Weight update using gradient descent\n",
    "$\n",
    "\\theta:=\\theta-\\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta}\n",
    "$\n",
    "5. We choose one-vs-rest strategy for multi-class classification where one class is assumed \"positive\" at one time and the rest are assumed \"negative\" and the processes is repeated while all the classes are covered.<br>\n",
    "6. We use iris data-set from url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "which has 3 species of iris plants.\n",
    "7. We use 4 features for each of the 3 classes\n",
    "8. During prediction in MLR, a data point is assigned to a class that gets maximum probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost for class Iris-setosa at iteration 0/3000 is 6.10343505996\n",
      "The cost for class Iris-setosa at iteration 1000/3000 is 0.00745789325876\n",
      "The cost for class Iris-setosa at iteration 2000/3000 is 0.00404383509684\n",
      "The cost for class Iris-versicolor at iteration 0/3000 is 4.087417799\n",
      "The cost for class Iris-versicolor at iteration 1000/3000 is 0.503953829955\n",
      "The cost for class Iris-versicolor at iteration 2000/3000 is 0.492574970468\n",
      "The cost for class Iris-virginica at iteration 0/3000 is 4.63631108073\n",
      "The cost for class Iris-virginica at iteration 1000/3000 is 0.104899054365\n",
      "The cost for class Iris-virginica at iteration 2000/3000 is 0.0756085362569\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def fit(x,y,max_itr=3000,alpha=0.1):\n",
    "    x = np.insert(x.astype(float),0,1,axis=1)\n",
    "    thetas = [] # holds weight vectors for the classes, data type: list\n",
    "    classes = np.unique(y)\n",
    "    costs = np.zeros(max_itr)\n",
    "    for c in classes: # loop over the classes\n",
    "        theta = np.random.rand(x.shape[1]) # weight initialization\n",
    "        binary_y = np.where(y==c,1,0) # one-vs-rest strategy for multiclass classification\n",
    "        for epoch in range(max_itr):\n",
    "            costs[epoch] = cost_function(theta,x,binary_y) # cost in each iteration is stored so that it can be plotted, the cost must show a decreasing trend\n",
    "            if np.remainder(epoch,1000)==0:\n",
    "                print(\"The cost for class {} at iteration {}/{} is {}\".format(c,epoch,max_itr,costs[epoch]))\n",
    "            grad   = gradient(theta,x,binary_y)\n",
    "            theta -= alpha * grad # weight update\n",
    "        thetas.append(theta)\n",
    "    return thetas,classes,costs\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1+np.exp(-z))\n",
    "def net_input(theta,x):\n",
    "    return np.dot(x,theta)\n",
    "def probablity(theta,x):\n",
    "    return sigmoid(net_input(theta,x))\n",
    "def cost_function(theta,x,y):\n",
    "    m = x.shape[0]\n",
    "    total_cost = -(1.0/m) * np.sum(y * np.log(probablity(theta,x))+(1-y)*np.log(1-probablity(theta,x)))\n",
    "    return total_cost\n",
    "def gradient(theta,x,y):\n",
    "    m = x.shape[0]\n",
    "    grad = (1.0)/m*np.dot(x.T,sigmoid(net_input(theta,x))-y)\n",
    "    return grad\n",
    "\n",
    "# load the data\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "df = pd.read_csv(url,header=None,names=[\n",
    "    \"Sepal length (cm)\",\n",
    "    \"Sepal width (cm)\", \n",
    "    \"Petal length (cm)\",\n",
    "    \"Petal width (cm)\",\n",
    "    \"Species\"\n",
    "]) # use 4 features\n",
    "#df.head()\n",
    "\n",
    "# get train and test data\n",
    "data = np.array(df)\n",
    "np.random.shuffle(data)\n",
    "num_train = int(.8*len(data)) # 80/20 train/test data split\n",
    "x_train, y_train = data[:num_train,:-1], data[:num_train,-1]\n",
    "x_test,  y_test  = data[num_train:,:-1], data[num_train:,-1]\n",
    "\n",
    "# train the model\n",
    "thetas, classes, costs = fit(x_train,y_train)\n",
    "\n",
    "# compute accuracy\n",
    "x_test1 = np.insert(x_test.astype(float),0,1,axis=1)\n",
    "numClasses = len(classes)\n",
    "p          = np.zeros([x_test1.shape[0],numClasses])\n",
    "for c in range(numClasses):\n",
    "    theta1 = np.array(thetas[c])\n",
    "    p[:,c] = probablity(theta1,x_test1)\n",
    "    \n",
    "pred_classes = np.argmax(p,axis=1) # choose the class with maximum probabilities for a data point\n",
    "y_test_numeric = np.zeros(y_test.shape)# give numeric label to each class for the computation of accuracy\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    for c in range(numClasses):\n",
    "        if(y_test[i]==classes[c]):\n",
    "            y_test_numeric[i]=c\n",
    "            \n",
    "accuracy = np.mean((y_test_numeric==pred_classes)) * 100.0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 86.67 %\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy is {} %\".format(np.round(accuracy*100.0)/100));\n",
    "# plt.plot(costs)\n",
    "# plt.xlabel('# EPOCHS')\n",
    "# plt.ylabel('COST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
